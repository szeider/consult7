# Example configuration for custom OpenAI-compatible providers
# Copy this file to 'providers.yaml' in your project root and customize as needed

custom_providers:
  # GitHub Copilot example
  - name: "github-copilot"
    display_name: "GitHub Copilot"
    api_base_url: "https://api.githubcopilot.com"
    feature_support:
      tool_calling: true
      json_mode: true
      streaming: true
      thinking_mode: false
    models:
      - name: "gemini-2.5-pro"
        context_length: 1000000
        max_output_tokens: 8192
      - name: "gpt-4.1"
        context_length: 1000000
        max_output_tokens: 8192
      - name: "o3-mini"
        context_length: 200000
        max_output_tokens: 8192

  # Groq example
  - name: "groq"
    display_name: "Groq"
    api_base_url: "https://api.groq.com/openai/v1"
    feature_support:
      tool_calling: true
      json_mode: true
      streaming: true
      thinking_mode: false
    models:
      - name: "llama-3.1-70b-versatile"
        context_length: 131072
        max_output_tokens: 8192
      - name: "llama-3.1-8b-instant"
        context_length: 131072
        max_output_tokens: 8192

  # Custom provider with parameter overrides
  - name: "custom-provider"
    display_name: "Custom AI Provider"
    api_base_url: "https://api.example.com/v1"
    feature_support:
      tool_calling: false
      json_mode: true
      streaming: false
      thinking_mode: false
    parameter_overrides:
      temperature: 0.7
      max_tokens: 4096
    models:
      - name: "custom-model-v1"
        context_length: 32768
        max_output_tokens: 4096
        parameter_overrides:
          temperature: 0.9  # Override global temperature for this model